{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T09:03:43.291698Z",
     "start_time": "2024-05-14T09:03:43.258922Z"
    }
   },
   "source": [
    "import os, json\n",
    "QnA_dir_path = 'final_1000_0425_human_checked.json'\n",
    "with open(QnA_dir_path, 'r') as f:\n",
    "    file_data = json.load(f)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# len(file_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T09:03:43.835632Z",
     "start_time": "2024-05-14T09:03:43.830303Z"
    }
   },
   "id": "f8e313c99b329cfa",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('/home/luhan/real_pubmedqa.json', 'r') as f:\n",
    "    ques = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:27:17.130715Z",
     "start_time": "2024-05-14T10:27:17.069713Z"
    }
   },
   "id": "a05061626a09ddce",
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer_list = [q['completion'] for q in ques]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:27:17.658006Z",
     "start_time": "2024-05-14T10:27:17.651889Z"
    }
   },
   "id": "dd97248aa65e1f77",
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'internlm2_20b_protein_lora_v3_pretrain_modified'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = \"/data/llm_models/huggingface/hub\"\n",
    "\n",
    "model_path = f'/data/llm_models/{model_name}'\n",
    "if 'models--' in model_name:\n",
    "    fs = f'/data/llm_models/huggingface/hub/{model_name}/snapshots/'\n",
    "    model_path = fs + os.listdir(f'/data/llm_models/huggingface/hub/{model_name}/snapshots/')[0]\n",
    "benchmark_size = 1000\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map = \"auto\").eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T09:04:37.516052Z",
     "start_time": "2024-05-14T09:03:46.246249Z"
    }
   },
   "id": "40a6a1f03bc7c988",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ques = ques[:500]\n",
    "chat_model = ('chat' in model_name) or ('Chat' in model_name)\n",
    "if 'Baichuan' in model_name:\n",
    "    chat_model = False\n",
    "inputs = []\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if ('struct' or 'it') in model_name:\n",
    "    for q in ques:\n",
    "        q = q['prompt']\n",
    "        a = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": q}], return_tensors=\"pt\").to(\"cuda\")\n",
    "        inputs.append(a)\n",
    "elif not chat_model:\n",
    "    for q in ques:\n",
    "        q = q['prompt'] + 'The corrct option is'\n",
    "        a = tokenizer(q, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = a.input_ids.to('cuda')\n",
    "        inputs.append(input_ids)\n",
    "else:\n",
    "    inputs = [q['prompt'] + 'The corrct option is' for q in ques]\n",
    "\n",
    "print(len(inputs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:27:22.180919Z",
     "start_time": "2024-05-14T10:27:21.426466Z"
    }
   },
   "id": "8fad6054dedd6ee7",
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "output_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:27:23.225072Z",
     "start_time": "2024-05-14T10:27:23.220128Z"
    }
   },
   "id": "780fe9203c8a3643",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-14T10:27:24.042593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "temp = 0.1\n",
    "mnt = 10\n",
    "for q in tqdm(inputs[len(output_list):]):\n",
    "    if chat_model:\n",
    "        try:\n",
    "            if 'Mistral' in model_name:\n",
    "                output_list.append(model.chat(tokenizer, q, do_sample=True, max_new_tokens=mnt, temperature=temp, history=[], eos_token_id=2, pad_token_id=2))\n",
    "            else:\n",
    "                output_list.append(model.chat(tokenizer, q, max_new_tokens=mnt, do_sample=True, temperature=temp, history=[]))\n",
    "        except:\n",
    "            output_list.append(model.generate(q, max_new_tokens=mnt, do_sample=True, temperature=temp))\n",
    "    else:\n",
    "        if 'Mistral' in model_name:\n",
    "            output_list.append(model.generate(q, max_new_tokens=mnt,do_sample=True, temperature=temp, eos_token_id=2, pad_token_id=2))\n",
    "        elif 'llama3-8B' or 'Llama-3' in model_name:\n",
    "            output_list.append(model.generate(q, max_new_tokens=mnt,do_sample=True, temperature=temp, eos_token_id=128001, pad_token_id=128001))\n",
    "        elif 'falcon' in model_name:\n",
    "            output_list.append(model.generate(q, max_new_tokens=mnt,do_sample=True, temperature=temp, eos_token_id=11, pad_token_id=11))\n",
    "        else:\n",
    "            output_list.append(model.generate(q, max_new_tokens=mnt, do_sample=True,temperature=temp))"
   ],
   "id": "56cad03cb7512fbd",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "question = [q['prompt'] for q in ques]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:25:49.406434Z",
     "start_time": "2024-05-14T10:25:49.396037Z"
    }
   },
   "id": "6d0c11b0ca1d5410",
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "after = []\n",
    "lst = [tokenizer.decode(i[0], skip_special_tokens=True) for i in output_list]\n",
    "if not chat_model:\n",
    "    for i, j in zip(lst, question):\n",
    "        after.append(i.replace(j, ''))\n",
    "else:\n",
    "    for i, j in zip(output_list, question):\n",
    "        after.append(i[0].replace(j, ''))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:25:50.649213Z",
     "start_time": "2024-05-14T10:25:50.513937Z"
    }
   },
   "id": "c960dfb1a7e20afa",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T10:25:51.777501Z",
     "start_time": "2024-05-14T10:25:51.773697Z"
    }
   },
   "cell_type": "code",
   "source": "# other = True",
   "id": "5a0404d22292dd5",
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# after = [a[15:]  for a in after]\n",
    "import re\n",
    "\n",
    "organized_output_list = []\n",
    "for i in after:  # Assuming 'after' is a list of strings you're iterating through\n",
    "    # if ('A) no' or 'B) yes') in i:\n",
    "    #     organized_output_list.append('x')\n",
    "    #     continue\n",
    "    # eos = '<|im_end|>'\n",
    "    # if eos in i:\n",
    "    #     index = i.rfind(eos) + len(eos)\n",
    "    #     i = i[index:].strip()\n",
    "    # if other:\n",
    "    #     for o in after:\n",
    "    #         try:\n",
    "    #             organized_output_list.append(re.search(r'\\d+', o).group())\n",
    "    #         except:\n",
    "    #             organized_output_list.append(\"None\")\n",
    "    #     break\n",
    "    if 'Answer' in i:\n",
    "        index = i.rfind('Answer') + len('Answer')\n",
    "        i = i[index:].strip()\n",
    "    temp = re.search(r'[A-D]',\n",
    "                     i)  # This regex matches the first uppercase letter\n",
    "    if temp:\n",
    "        organized_output_list.append(\n",
    "            temp.group())  # Append the first uppercase letter found\n",
    "    else:\n",
    "        organized_output_list.append(\n",
    "            'x')  # Append 'x' if no uppercase letter is found"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:25:52.448347Z",
     "start_time": "2024-05-14T10:25:52.437071Z"
    }
   },
   "id": "a412598fef292507",
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "num_correct = 0\n",
    "for i in range(len(organized_output_list)):\n",
    "    if organized_output_list[i] == answer_list[i]:\n",
    "        num_correct += 1\n",
    "num_correct / len(organized_output_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:25:55.806849Z",
     "start_time": "2024-05-14T10:25:55.796219Z"
    }
   },
   "id": "6f98115f00b84de8",
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# inputs = []\n",
    "# for q in question:\n",
    "#     a = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": q}], return_tensors=\"pt\").to(\"cuda\")\n",
    "#     inputs.append(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T10:08:14.814892Z",
     "start_time": "2024-03-29T10:08:14.811643Z"
    }
   },
   "id": "3214162c2e377b18",
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29c0faee59ffdc5b",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
